package server

import (
	"database/sql"
	"fmt"

	//"fmt"
	"strings"

	"github.com/BurntSushi/migration"
	serviceconfig "github.com/IMQS/serviceconfigsgo"
)

// ROWID is the default row id field for the IMQS domain
const ROWID = "rowid"

func _pgdbAction(conf *serviceconfig.DBConfig, action string) error {
	// Connect via the Postgres database, so that we can create the DB
	cfg2 := *conf
	cfg2.Name = "postgres"
	db, err := sql.Open(conf.Driver, cfg2.DSN())
	if err != nil {
		return err
	}
	defer db.Close()
	logicalOperator := ""
	if !strings.EqualFold("create", action) {
		logicalOperator = "IF EXISTS"
	}
	actions := []string{
		action + " DATABASE " + logicalOperator + " \"" + conf.Name + "\"",
	}
	if strings.EqualFold("drop", action) {
		actions = append(
			[]string{"SELECT pg_terminate_backend(pid) from pg_stat_activity where datname='" + conf.Name + "'"},
			actions...,
		)
	}
	for _, _action := range actions {
		if _, eExec := db.Exec(_action); eExec != nil {
			return fmt.Errorf("Could not execute SQL command:\nERROR: %v\nSQL: %v", eExec, _action)
		}
	}

	return nil
}

func createDB(conf *serviceconfig.DBConfig) error {
	return _pgdbAction(conf, "CREATE")
}

func isDBNotExistError(err error, dbName string) bool {
	return strings.Index(err.Error(), "pq: database \""+dbName+"\" does not exist") != -1
}

func dropDB(conf *serviceconfig.DBConfig) error {
	return _pgdbAction(conf, "DROP")
}

func escapeSQLIdent(db *sql.DB, ident string) string {
	return "\"" + ident + "\""
}

func escapeSQLLiteral(db *sql.DB, literal string) string {
	return "'" + strings.Replace(literal, "'", "''", -1) + "'"
}

func createMigrations(genericCfg *serviceconfig.DBConfig) []migration.Migrator {
	var migrations []migration.Migrator

	// Each of these migrations corresponds to a version in whole migration.
	// If a migration has more than one SQL statement, you must separate them with semicolons.

	/*
		--- Migration 1 ---
		We have two options here:
		1. No primary key on searchindex.
		   Keep an index on token
		   Keep an index on srctab
		2. Primary key is (token, srctab, srcrow)
		   Keep an index on srctab

		Statistics, with 1 million records, as generated by the script inside benchmark.go
		Times are a single sample, after 3 runs.
		1. Table Size 52 MB. Indexes Size 120 MB. Query 0.235 ms. Populate swift (420k records) 9.5 seconds
		2. Table Size 52 MB. Indexes Size 124 MB. Query 0.235 ms. Populate swift (420k records) 9.1 seconds

		The numbers are pretty equal.
		I'm going with option (2) since it helps us make sure we don't screw up, by enforcing key uniqueness.

		This is the schema for option (1)
		CREATE TABLE searchindex(token VARCHAR, srctab INTEGER, srcrow BIGINT);
		CREATE INDEX idx_searchindex_token on searchindex(token);
		CREATE INDEX idx_searchindex_srctab on searchindex(srctab);
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`CREATE TABLE searchindex(token VARCHAR, srctab INTEGER, srcrow BIGINT, PRIMARY KEY(token, srctab, srcrow));
		CREATE INDEX idx_searchindex_srctab ON searchindex(srctab);
		CREATE TABLE searchtables(tablename VARCHAR, srctab INTEGER, PRIMARY KEY(tablename));
		CREATE INDEX idx_searchtables_srctab ON searchtables(srctab);`))

	/*
		--- Migration 2 ---
		Added modstamp
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`DELETE FROM searchindex;
		DELETE FROM searchtables;
		ALTER TABLE searchtables ADD COLUMN modstamp VARCHAR;`))

	/*
		--- Migration 3 ---
		Why COLLATION "C"? Without that, we can't generate ranges reliably. For example, '9' + 1 = ':',
		but the default Postgres collation doesn't agree with that. By using the "C" collation, we get
		to treat strings as simply binary byte strings.
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`DELETE FROM searchtables;
		DROP TABLE searchindex;
		CREATE TABLE searchindex(token VARCHAR COLLATE "C", srctab INTEGER, srcrow BIGINT, PRIMARY KEY(token, srctab, srcrow));
		CREATE INDEX idx_searchindex_srctab ON searchindex(srctab);`))

	/*
		--- Migration 4 ---
		Automatic vacuums are taking down some old servers with HDDs. SSD servers are fine.
		See 'doc.go' for more.
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`ALTER TABLE searchindex SET (autovacuum_enabled = false);`))

	/*
		--- Migration 5 ---
		Fixup: srctab must be unique
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`DROP INDEX idx_searchtables_srctab;
		CREATE UNIQUE INDEX idx_searchtables_srctab ON searchtables(srctab);`))

	/*
		--- Migration 6 ---
		Upgrade the 'srctab' concept to the 'srcfield' concept. A 'srcfield'
		is (srctab << 16 | srcfield). It allows us to limit our search results by field.
		Introduced the concept of searchnametable, which is a generic lookup table from
		string to small integer. We use this for both table names and field names, so that
		we can form a unique integer for every field (ie srctab << 16 | srcfield mentioned above).
		Change naming convention to use underscores.
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`DROP TABLE searchindex;
		DROP TABLE searchtables;
		CREATE TABLE search_index(token VARCHAR COLLATE "C", srcfield INTEGER, srcrow BIGINT, PRIMARY KEY(token, srcfield, srcrow));
		CREATE INDEX idx_search_index_srcfield ON search_index(srcfield);
		ALTER TABLE search_index SET (autovacuum_enabled = false);
		CREATE TABLE search_nametable(name VARCHAR, id INTEGER, PRIMARY KEY(name));
		CREATE UNIQUE INDEX idx_search_nametable_id ON search_nametable(id);
		CREATE TABLE search_src_tables(tablename VARCHAR, modstamp VARCHAR, PRIMARY KEY(tablename));`))

	/*
		--- Migration 7 ---
		Change (srcrow BIGINT) to (srcrows BYTEA), so that we can represent relationships tables.
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`DROP TABLE search_index;
		CREATE TABLE search_index(token VARCHAR COLLATE "C", srcfield INTEGER, srcrows BYTEA, PRIMARY KEY(token, srcfield, srcrows));
		CREATE INDEX idx_search_index_srcfield ON search_index(srcfield);
		ALTER TABLE search_index SET (autovacuum_enabled = false);`))

	/*
		--- Migration 8 ---
		Create new table that will store additional config settings to the file-based config.
		We can now store configuration settings inside the database (in addition to the JSON config file)
	*/
	migrations = append(migrations, makeMigrationFromSQL(
		`CREATE TABLE search_config (dbname VARCHAR, tablename VARCHAR, config JSONB, PRIMARY KEY (dbname, tablename));`))

	/*
		--- Migration 9 ---
		In the generic system, every time there is a new import,
		a new table name is reserved for each of the files contained in the import.
		That name which will look like, for example g_table_2, where the last digit is incremented for each new table,
		will change with each import of the same file. So to keep track of all these changes we created another
		field called the tablename_external in the IMQS_METATABLE on the generic database which keeps tracks of the import file name.

		The issue then becomes what should happen to the already indexed fields for that generic table which are being
		stored in the search_config table, when the generic file is reimported or deleted from the original database?
		Thats why we have added a new field to the search_config table called the long_lived_name. Just like
		the table_external_name in the IMQS MetaTable table, this field will store the original name of the table
		and we expect it to never change between diffent re-imports.

		We can update or delete any search_config using long_lived_name field as a reference
	*/
	migrations = append(migrations, func(tx migration.LimitedTx) error {
		migrationSQL := `ALTER TABLE search_config DROP CONSTRAINT search_config_pkey;`
		migrationSQL += `ALTER TABLE search_config ADD COLUMN longlived_name VARCHAR;`

		genericDB, err := sql.Open(genericCfg.Driver, genericCfg.DSN())
		if err != nil {
			return err
		}
		defer genericDB.Close()

		// Check if ImqsMetaTable exists.
		// This check was added for unit tests.
		// However, I think it might also prove useful in future, when services are started up on a clean server.
		// For example, imagine a virgin server, with no databases created yet. If the search service starts up before
		// the Generic service, then the ImqsMetaTable won't exist yet. So this seems like a legitimate check to
		// keep in here.
		var nMetaTable int64
		if err := genericDB.QueryRow(`SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ImqsMetaTable'`).Scan(&nMetaTable); err != nil {
			return err
		}

		if nMetaTable == 1 {
			metaRows, err := genericDB.Query(`SELECT "TableNameInternal", "TableNameExternal" FROM "ImqsMetaTable"`)
			if err != nil {
				return err
			}
			defer metaRows.Close()

			for metaRows.Next() {
				var internalName, externalName string
				if err := metaRows.Scan(&internalName, &externalName); err != nil {
					return err
				}
				migrationSQL += `UPDATE search_config SET longlived_name = '` + externalName + `' WHERE tablename = '` + internalName + `'; `
			}
			metaRows.Close()

			// On InfraQA, we discovered that many rows inside search_config referred to tables that were no longer present in the Generic DB.
			// This is expected, because until now, we've had no way of keeping the search_config table up to date, when Generic imports
			// were added or modified.
			// Our solution is to just get rid of the invalid entries
			migrationSQL += `DELETE FROM search_config WHERE longlived_name IS NULL;`
		}

		migrationSQL += `ALTER TABLE search_config ADD CONSTRAINT search_config_pkey PRIMARY KEY (dbname, longlived_name);`

		_, err = tx.Exec(migrationSQL)
		return err
	})

	return migrations
}

func makeMigrationFromSQL(sql string) migration.Migrator {
	return func(tx migration.LimitedTx) error {
		_, err := tx.Exec(sql)
		return err
	}
}
